# 决策树
- 决策树（decision tree）是一类常见的机器学习的方法，一二分类任务为例
  决策树是基于树结构来进行决策的，这恰好是人类在面临决策问题是一种很
  自然的处理机制，
- 一般的，一棵决策树包含一个根结点，若干个内部结点和若干个叶节点，叶节点
  对应于决策树结果，其他每一个结点则对应于一个属性测试，每一个结点包含
  的样本集合根据属性测试的结果被划分到了子结点中，根节点包含样本全集，
  从根节点到每一个叶节点的路径对应了一个判定测试序列，决策树学习的目的
  是为了产生一棵泛化能力强，即处理未见示例能力强的决策树，其基本流程
  遵循简单且直观的“分而治之”（divide-and-conquer）策略
  
  
  
# 在使用了，最基础的编程实现以后，在使用sklearn包来实现，决策树
    - sklearn.tree模块提供了，决策树模型，，用于分类问题和回归问题
    - 下面就是官方文档
    - http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html
- 在这次的实战当中使用到的函数就是：
    - DecisionTreeClassifier和 export_graphviz，
    - 前者用于决策树构建，后者用于决策树可视化

- 下面就是对上面两个函数的官方解读：

# DecisonTreeClassifier函数：
- 该函数一共有12个参数：
    - criterion：
            - 特征选择标准，可选参数，默认是gini，可以设置为entropy(熵)。
              gini是基尼不纯度，是将来自集合的某种结果随机应用于某一数据项的
              预期误差率,是一种基于统计的思想。entropy是香农熵，也就是上篇文章讲过的内容，
              是一种基于信息论的思想。Sklearn把gini设为默认参数，应该也是做了相应的斟酌的，
              精度也许更高些？ID3算法使用的是entropy，CART算法使用的则是gini。
    - splitter：
            - 特征划分点选择标准，可选参数，默认是best(最优特征)，可以设置为random。
              每个结点的选择策略。best参数是根据算法选择最佳的切分特征，
              例如gini、entropy。random随机的在部分划分点中找局部最优的划分点。
              默认的”best”适合样本量不大的时候，而如果样本数据量非常大，
              此时决策树构建推荐”random”。
    - max_features：
            - 划分时考虑的最大特征数，可选参数，默认是None。
              寻找最佳切分时考虑的最大特征数(n_features为总共的特征数)，
              有如下6种情况：
                 - 如果max_features是整型的数，则考虑max_features个特征；
                 - 如果max_features是浮点型的数，则考虑int(max_features * n_features)个特征；
                 - 如果max_features设为auto，那么max_features = sqrt(n_features)；
                 - 如果max_features设为sqrt，那么max_featrues = sqrt(n_features)，跟auto一样；
                 - 如果max_features设为log2，那么max_features = log2(n_features)；
                 - 如果max_features设为None，那么max_features = n_features，也就是所有特征都用。
                 - 一般来说，如果样本特征数不多，比如小于50，我们用默认的”None”就可以了，
                   如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的
                   最大特征数，以控制决策树的生成时间。
    - max_depth：
            - 决策树最大深度，可选参数，默认是None。这个参数是这是树的层数的.层数的
              概念就是，比如在贷款的例子中，决策树的层数是2层.如果这个参数设置为None，
              那么决策树在建立子树的时候不会限制子树的深度.一般来说，数据少或者特征
              少的时候可以不管这个值,或者如果设置了min_samples_slipt参数，那么直到
              少于min_smaples_split个样本为止。如果模型样本量多，特征也多的情况下，
              推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-
              100之间。

    - min_samples_split：
            - 内部节点再划分所需最小样本数，可选参数，默认是2。这个值限制了子树继续
              划分的条件。如果min_samples_split为整数，那么在切分内部结点的时候，
              min_samples_split作为最小的样本数，也就是说，如果样本已经少于
              min_samples_split个样本，则停止继续切分。如果min_samples_split为浮点数，
              那么min_samples_split就是一个百分比，ceil(min_samples_split * n_samples),
              数是向上取整的。如果样本量不大，不需要管这个值。如果样本量数量级非常大，
              则推荐增大这个值。
    - min_weight_fraction_leaf：
            - 叶子节点最小的样本权重和，可选参数，默认是0,这个值限制了叶子节点所有
              样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。一般来说，
              如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入
              样本权重，这时我们就要注意这个值了。
    - max_leaf_nodes：
            - 最大叶子节点数，可选参数，默认是None。通过限制最大叶子节点数，可以防止
              过拟合,如果加了限制，算法会建立在最大叶子节点数内最优的决策树,如果特
              征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的
              值可以通过交叉验证得到。
    - class_weight：
            - 类别权重，可选参数，默认是None，也可以字典、字典列表、balanced。指定样
              本各类别的的权重，主要是为了防止训练集某些类别的样本过多，导致训练的决
              策树过于偏向这些类别。类别的权重可以通过{class_label：weight} 这样的格
              式给出，这里可以自己指定各个样本的权重，或者用balanced(平衡模式)，如果
              使用balanced则算法会自己计算权重，样本量少的类别所对应的样本权重会高。
              当然，如果你的样本类别分布没有明显的偏倚，则可以不管这个参数，选择默认
              的None。
    - random_state：
            - 可选参数，默认是None。随机数种子。如果是证书，那么random_state会作为随
              机数生成器的随机数种子。随机数种子，如果没有设置随机数，随机出来的数与
              当前系统时间有关，每个时刻都是不同的。如果设置了随机数种子，那么相同随
              机数种子，不同时刻产生的随机数也是相同的。如果是RandomState instance，
              那么random_state是随机数生成器。如果为None，则随机数生成器使用np.random。
    - min_impurity_split：
            - 节点划分最小不纯度,可选参数，默认是1e-7。这是个阈值，这个值限制了决策树
              的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈
              值，则该节点不再生成子节点。即为叶子节点 。
    - presort：(也可以是预分类)
            - 数据是否预排序，可选参数，默认为False，这个值是布尔值，默认是False不排序。
              一般来说，如果样本量少或者限制了一个深度很小的决策树，设置为true可以让划
              分点选择更加快，决策树建立的更加快。如果样本量太大的话，反而没有什么好处。
              问题是样本量少的时候，我速度本来就不慢。所以这个值一般懒得理它就可以了。

# 除了以上的参数，我们在调参是应该还要注意这些问题：
    - 当样本数量少但是样本特征非常多的时候，决策树很容易过拟合，一般来说，样本数比
      特征数多一些会比较容易建立健壮的模型
    - 如果样本数量少但是样本特征非常多，在拟合决策树模型前，推荐先做维度规约，比如
      主成分分析（PCA），特征选择（Losso）或者独立成分分析（ICA）。这样特征的维度
      会大大减小。再来拟合决策树模型效果会好
    - 推荐多用决策树的可视化，同时先限制决策树的深度，这样可以先观察下生成的决策树
      里数据的初步拟合情况，然后再决定是否要增加深度.
    - 在训练模型时，注意观察样本的类别情况（主要指分类树），如果类别分布非常不均匀，
      就要考虑用class_weight来限制模型过于偏向样本多的类别.
    - 决策树的数组使用的是numpy的float32类型，如果训练数据不是这样的格式，算法会先
      做copy再运行
    - 如果输入的样本矩阵是稀疏的，推荐在拟合前调用csc_matrix(matrix=矩阵)稀疏化，在
      预测前调用csr_matrix 稀疏化
      
# sklearn.tree.DescisionClassifier()提供了一些方法：
    - apply(X[, check_input])	
        Returns the index of the leaf that each sample is predicted as.
    - decision_path(X[, check_input])	
        Return the decision path in the tree
    - fit(X, y[, sample_weight, check_input, …])	
        Build a decision tree classifier from the training set (X, y).
    - get_params([deep])	
        Get parameters for this estimator.
    - predict(X[, check_input])	
        Predict class or regression value for X.
    - predict_log_proba(X)	
        Predict class log-probabilities of the input samples X.
    - predict_proba(X[, check_input])	
        Predict class probabilities of the input samples X.
    - score(X, y[, sample_weight])	
        Returns the mean accuracy on the given test data and labels.
    - set_params(**params)	
        Set the parameters of this estimator.
        
        
# 下面就是关于一席函数的使用，与区别：
- 1. fit()函数：
    - fit():  Method calculates the parameters μ and σ and saves them as internal objects.
      解释：简单来说，就是求得训练集X的均值，方差，最大值，最小值,这些训练集X固有的属性。
      
    - transform(): Method using these calculated parameters apply the transformation to 
                   a particular dataset.
             解释：在fit的基础上，进行标准化，降维，归一化等操作（看具体用的是哪个工具，
                   如PCA，StandardScaler等）。
    - fit_transform(): joins the fit() and transform() method for transformation of dataset.
                 解释：fit_transform是fit和transform的组合，既包括了训练又包含了转换。
    - transform()和fit_transform()二者的功能都是对数据进行某种统一处理（比如标准化~N(0,1)，
                   将数据缩放(映射)到某个固定区间，归一化，正则化等）      
    - fit_transform(trainData)对部分数据先拟合fit，找到该part的整体指标，
                   如均值、方差、最大值最小值等等（根据具体转换的目的），然后对该trainData
                   进行转换transform，从而实现数据的标准化、归一化等等。
    - 这是关于其使用比较的文档：博客              
    - https://blog.csdn.net/weixin_38278334/article/details/82971752
    
    
- 2.LabelEncoder： 将字符串转换为增量值（0123....）
    OneHotEncoder：使用One-of-K算法将字符串转换为整数  
    - 这就是官方文档：
    - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html            
     
       
#  总结
    - 决策树的一些优点：
      - 易于理解和解释，决策树可以可视化。
        几乎不需要数据预处理。其他方法经常需要数据标准化，创建虚拟变量和删除缺失值。
      - 决策树还不支持缺失值。
      - 使用树的花费（例如预测数据）是训练数据点(data points)数量的对数。
      - 可以同时处理数值变量和分类变量。其他方法大都适用于分析一种变量的集合。
      - 可以处理多值输出变量问题。
      - 使用白盒模型。如果一个情况被观察到，使用逻辑判断容易表示这种规则。相反，如果是黑盒模型（例如人工神经网络），结果会非常难解释。
      - 即使对真实模型来说，假设无效的情况下，也可以较好的适用。
    - 决策树的一些缺点：
      - 决策树学习可能创建一个过于复杂的树，并不能很好的预测数据。也就是过拟合。
        修剪机制（现在不支持），设置一个叶子节点需要的最小样本数量，或者数的最大深度，
        可以避免过拟合。
      - 决策树可能是不稳定的，因为即使非常小的变异，可能会产生一颗完全不同的树。这个问题通过decision trees with an ensemble来缓解。
      - 学习一颗最优的决策树是一个NP-完全问题under several aspects of optimality and even for simple concepts。
        因此，传统决策树算法基于启发式算法，例如贪婪算法，即每个节点创建最优决策。这些算法不能产生一个全家最优的决策树。对样本和特征随机抽样可以降低整体效果偏差。
        概念难以学习，因为决策树没有很好的解释他们，例如，XOR, parity or multiplexer problems.
      - 如果某些分类占优势，决策树将会创建一棵有偏差的树。因此，建议在训练之前，先抽样使样本均衡。



  
  
  
  
  
  
  
  
  
  
