# sklearn.linear_model提供了很多线性模型
- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model
- 包括岭回归、贝叶斯回归、Lasso等。
- 本文主要讲解岭回归Ridge。
- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html

- 模型的介绍：
    - class sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True, 
      normalize=False, copy_X=True, max_iter=None, tol=0.001, 
      solver=’auto’, random_state=None)
    
- Ridge函数介绍：
    - alpha：正则化系数，float类型，默认为1.0。正则化改善了问题的条件
             并减少了估计的方差。较大的值指定较强的正则化。
    - fit_intercept：是否需要截距，bool类型，默认为True。也就是是否
                     求解b。
    - normalize：是否先进行归一化，bool类型，默认为False。如果为真，
                 则回归X将在回归之前被归一化。 当fit_intercept设置为
                 False时，将忽略此参数。 当回归量归一化时，注意到这使
                 得超参数学习更加鲁棒，并且几乎不依赖于样本的数量。 
                 相同的属性对标准化数据无效。然而，如果你想标准化，
                 请在调用normalize = False训练估计器之前，使用
                 preprocessing.StandardScaler处理数据。
    - copy_X：是否复制X数组，bool类型，默认为True，如果为True，将复制X
              数组; 否则，它覆盖原数组X。
    - max_iter：最大的迭代次数，int类型，默认为None，最大的迭代次数，
                对于sparse_cg和lsqr而言，默认次数取决于
                scipy.sparse.linalg，对于sag而言，则默认为1000次。
    - tol：精度，float类型，默认为0.001。就是解的精度。
    - solver：求解方法，str类型，默认为auto。可选参数为：
              auto、svd、cholesky、lsqr、sparse_cg、sag。
         - auto     根据数据类型自动选择求解器。
         - svd      使用X的奇异值分解来计算Ridge系数。对于奇异矩阵比
                     cholesky更稳定。
         - cholesky  使用标准的scipy.linalg.solve函数来获得闭合形式的解。
         - sparse_cg 使用在scipy.sparse.linalg.cg中找到的共轭梯度求解器。
                     作为迭代算法，这个求解器比大规模数据（
                     设置tol和max_iter的可能性）的cholesky更合适。
         - lsqr      使用专用的正则化最小二乘常数
                     scipy.sparse.linalg.lsqr。它是最快的，
                     但可能在旧的scipy版本不可用。它是使用迭代过程。
         - sag       使用随机平均梯度下降。它也使用迭代过程，并且
                     当n_samples和n_feature都很大时，通常比其他求解器
                     更快。注意，sag快速收敛仅在具有近似相同尺度的特征
                     上被保证。您可以使用sklearn.preprocessing的缩放器
                     预处理数据。
    - random_state：sag的伪随机种子。

